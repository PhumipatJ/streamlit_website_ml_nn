# -*- coding: utf-8 -*-
"""CNN_Valorant_Skins.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uyCKwIb1jiZCUIcgWxS5uw4U37bE1fA2
"""

from google.colab import drive
drive.mount('/content/drive')

import os

data_dir = "/content/drive/MyDrive/valorant_skins_dataset"  # Change to your dataset path


print("Weapon Categories:", os.listdir(data_dir))

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory


# Set up batch size and image size
batch_size = 32
img_size = (224, 224)  # Resize images

# Set a seed for reproducibility
seed = 123  # You can use any integer value here

# Load the training dataset (80% of the data, for example)
train_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,  # 20% data for validation
    subset="training",
    shuffle=True,
    batch_size=batch_size,
    image_size=img_size,
    seed=seed  # Provide the seed value here
)

# Load the validation dataset (20% of the data, for example)
test_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,  # 20% data for validation
    subset="validation",
    shuffle=True,
    batch_size=batch_size,
    image_size=img_size,
    seed=seed  # Provide the seed value here
)

# Check class names (weapon types)
class_names = train_ds.class_names
print("Classes:", class_names)

normalization_layer = tf.keras.layers.Rescaling(1./255)

train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))

from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(class_names), activation='softmax')  # 18 classes
])

model.summary()

from tensorflow.keras.optimizers import Adam

model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

import tensorflow as tf

# Create the EarlyStopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # Monitor the validation loss
    patience=3,  # Stop as soon as the validation loss does not improve
    restore_best_weights=True  # Restore the best model weights (i.e., the one with the lowest val_loss)
)

# Train the model with early stopping
epochs = 10
history = model.fit(
    train_ds,
    epochs=epochs,
    validation_data=test_ds,
    batch_size=32,
    callbacks=[early_stopping]  # Add the EarlyStopping callback
)

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(test_ds)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

# Get predictions from the model
yp = model.predict(test_ds)

model.save("valorant_skin_recognition.h5")

from google.colab import files
files.download("valorant_skin_recognition.h5")

import matplotlib.pyplot as plt
# Choose a specific index (e.g., the 10th image in the dataset)
index = 10
image_batch, label_batch = list(test_ds.take(1))[0]  # Take the first batch
img = image_batch[index].numpy().astype("float32")

# Scale the image to [0, 255] for display
img = (img * 255).astype("uint8")

# Get the predicted label for this image
yp = model.predict(image_batch)
predicted_label = class_names[yp[index].argmax()]

# Get the actual label for this image
actual_label = class_names[label_batch[index].numpy()]

# Display the image
plt.imshow(img)
plt.axis('off')  # Hide axes
plt.show()

# Print the actual and predicted labels
print(f"Actual label: {actual_label}")
print(f"Predicted label: {predicted_label}")